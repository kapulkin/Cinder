/**

\page Audio Audio

\tableofcontents

This document provides an overview of the audio capabilities in cinder. You can use this, along with the samples in the *samples/_audio* folder, as a jumping off point into the `ci::audio` namespace.

\section design Design

The `ci::audio` namespace consists of a number of different components and layers of abstraction:

- a modular, Node-based set of real-time audio tools. 
- Voice - a higher level 'simple api' for when all you want is to play back a sound file or get a processing function.
- audio file reading and writing tools, following cinder's Source / Target pattern. 
- A lower level set of DSP tools, like FFT transform, Biquad filtering and vector based math operations.

The core of the design draws from concepts found in other popular modular audio API's, namely [Web Audio][webaudio] and [Pure Data][puredata], while combining many of cinder's existing design patterns. We also take full advantage of C++11 features such as smart pointers, `std::atomic`'s, and `std::mutex`'s.  

A modular api is advantagous because it is proven to be very flexible and allows for reusability without a significant loss in performance.  Still, higher level constructs exist and more will be added as time permits.  The cinder philosophy remains, "easy things easy and hard things possible." 

\section voice_api Voice API

For those who need only to play back a sound file or want a simple processing function, you may not need to look further than what is provided in the Voice API.  Voice's are high level objects that sit above the modular system, managing a small chain of audio Node's that perform the necessary processing.  They are meant to require little startup and management. 

The following is an example of how to play a sound file with an `audio::Voice`:

    SourceFileRef sourceFile = audio::load( loadResource( "soundfile.wav" ) );
    mVoice = audio::Voice::create( sourceFile );
    mVoice->start();

The `Voice::create` method returns a shared_ptr to a Voice sub-class (ex, in the case of playing an audio file, returns `VoiceSamplePlayerRef`, but the user will generally only need to maintain this by storing it in a VoiceRef instance variable. It is necessary to store the returned VoiceRef because once it goes out of scope, it will be disconnected from the audio graph and destroyed.  This is fairly cheap, however (much cheaper than creating the `SourceFile` via `audio::load()`, for example), so if you need to later play a different file, you can safely call `Voice::create()` again and assign it to your `mVoice` instance variable.

Common tasks like `start()`, `stop()`, and `pause()` are supported. Each Voice has controls for volume and 2d panning.  Here is an example of how you'd control these with the mouse:

    void MyApp::mouseDown( app::MouseEvent event )
    {
    	mVoice->setVolume( 1.0f - (float)event.getPos().y / (float)getWindowHeight() );
    	mVoice->setPan( (float)event.getPos().x / (float)getWindowWidth() );
    }

\subsection voice_nodes How a Voice manages its Nodes

_note: It may be beneficial to skip to the Modular API documentation before continuing on in this section._

The Voice API sits above and ties into the modular API, which is explained below. Each Voice has a virtual `Voice::getInputNode()` member function that returns the `Node` object that is responsible for generating samples. This gives you access to more advanced functionality, like setting extended properties.

Because the Voice internally manages a chain of Node's, the actual `Node` that is connected to the master output can be retrieved with the virtual `Voice::getOutputNode()` member function.  This is currently always the `Pan2d`, however a generic `NodeRef` is returned because the actual type should be opaque to the user.

\subsection voice_direct Manually directing the Voice output

By default, a `Voice` is automatically connected up to `Context::master()`, which represents the active output hardware device (ex. speakers). If you plan to direct the `Voice`'s output to something else, you need to specify this at creation time by passing `false` to `Voice::Options::connectToMaster()`. For example, the following manually connects the `Voice` up to a `Scope` (which allows one to visualize the audio samples, see docs later on), and then to `Context::master()`:

    auto options = audio::Voice::Options().connectToMaster( false );
    mVoice = audio::Voice::create( sourceFile, options );
    mScope = audio::master()->makeNode( new audio::Scope );
  
    mVoice->getOutputNode() >> mScope >> audio::master()->getOutput();

See Also:

- [VoiceBasic](../../samples/_audio/VoiceBasic/src/VoiceBasicApp.cpp)
- [VoiceBasicProcessing](../../samples/_audio/VoiceBasicProcessing/src/VoiceBasicProcessingApp.cpp)

\section Audio Buffers

TODO: consider moving after Modular API section

Before jumping into the meat and potatoes, its a good idea to have an brief understanding of how `audio::Buffer`'s work, and a few of the `audio::Buffer` variants. For details, please see the header at `cinder/audio/Buffer.h`. Audio data is usually handled in two different manners:

1. a contigious stream of 'blocks' of equal length, which is how real-time processing is achieved.
2. as a single unit, such as the uncompressed data of sound file.

In both cases, there is a number of frames and a channel count that make up the layout of the Buffer.  Usually Buffer's used for case 1 are passed by pointer to reduce copying.  In case 2, they are either passed around as shared_ptr's (`BufferRef`'s), though this isn't required. Care is taken in multi-threaded situations by making defensive copies where appropriate.

It is also worth noting that samplerate is not a property of the buffer data - this is to allow for flexibility of how the data is interpreted.  Instead, samplerate is determined from the context in which the audio data is being used.

### Layout

The standard `Buffer` class used when passing audio between different components stores its channels contigiously (non-interleaved). This means that right after channel 0 (ex. left speaker) ends, the next sample in the internal array is the first sample of channel 1 (ex. right speaker). For convenience, the `Buffer::getChannel( size_t )` method with return a float* offset to the provided channel index, and also does some debug assertions that the index is in bounds. In the rare cases that interleaved audio is required (ex. interfacing with third-party or system level API's), there is a `BufferInterleaved` variant. 

Currently all processing is handled in single floating point precision.  However, the Buffer class is actually a typedef'ed `BufferT<float>`, so if a different sample format is required, one can use the BufferT class directly and provide the sample type.  While this will not by directly interchangeable with public interfaces in the `ci::audio` namespace, there are conversion utilities provided in `cinder/audio/dsp/Converter.h`. 

In all `Buffer` variants, you can get at the actual data store with the `Buffer::getData()` method, which returns a pointer to an array and can be used however the user wishes, provided they understand the buffer layout.

### Resizing

The standard `Buffer` class is not resizeable once constructed. However, sometimes it is necessary or more efficient to resize a buffer after construction, which is facilitited by the `BufferDynamic*` variants.  `BufferDynamic` has the methods, `setNumFrames()`, `setNumChannels()`, and `setSize()` (for both frames and channels at the same time), which will realloc the internal data store if required.  If the new size (frames * channels) is smaller or equal to the previous size, no reallocation will occur - by default `BufferDynamic` will only grow in order to prevent runtime reallocations where possible.  If you would like to free up extra space, you can use the provided `shrinkToFit()` method. 

\section modular Modular API

At the core of the `ci::audio` architecture is a modular, Node-based system that allows you to interconnect audio building blocks, such as sample players, waveform generators or effects, in a flexible manner appropriate to a specific application. A large influence of cinder's design comes from the [Web Audio spec][web_audio_spec] and for those interested, it has a fantastic introduction to [modular routing][web_audio_modular_routing] and the motivations behind it. 

\subsection context Context

The Context class manages platform specific audio processing and thread synchronization between the 'audio' (real-time) and 'user' (typically UI/main, but not limited to) threads. There is one 'master', which is the only hardware-facing Context. All Node's are created using the Context, which is necessary for thread synchronization.  A new Node is created like the following: 

    auto ctx = audio::Context::master();
    mNode = ctx->makeNode( new NodeType );


There are a couple important parameters governed by the current Context, which Node's use to configure their internal buffer layout:

_samplerate:_ the number of processing samples per second, typically 44,100 or 48,000.
_frames per block:_: processing is sectioned up into blocks (an array of numbers, usually a power of two) to facilitate real-time operations. The default value is 512, though some systems can go much lower (like mac).

These parameters are ultimately configured by the Context's `OutputNode` (accessible with `Context::getOutput()`), which is currently always of type `OutputNodeDevice`.  This means that the samplerate and frames-per-block settings are governed by your system's hardware settings.

It is worth noting that these values can change at runtime either by the user or system, which will cause all Nodes within a Context to be reconfigured.  This should in general 'just work', though authors of Node's should be aware of this possibility when deciding how to manage their Buffers.

\subsection node Node

A Node is the fundamental building block for audio processing graphs. They allow for flexible combinations of synthesis, analysis, effects, file reading/writing, etc., and are meant to be easily subclassed. There are a two important Node types also worth mentioning upfront:

- **OutputNode**: an endpoint at the end of an audio graph. Has no outputs.
- **InputNode**: an endpoint at the beginning of an audio graph. Has no inputs.

Node's are connected together to from an audio graph. For audio to reach the speakers, the last Node in the graph is connected to the Context's OutputNode:

    auto ctx = audio::Context::master();
    mSine = ctx->makeNode( new audio::GenSineNode );
    mGain = ctx->makeNode( new audio::GainNode );
    
    mSine->connect( mGain );
    mGain->connect( ctx->getOutput() );

Node's are connected from source to destination. A convenient shorthand syntax that is meant to represent this is as follows:

    mSine >> mGain >> ctx->getOutput();

To process audio, each Node subclass implements a virtual method `process( Buffer *buffer )`. Processing can be enabled or disabled on a per-Node basis. While `NodeEffect`s are enabled by default, `NodeInput`s must be turned on before they produce any audio. `OutputNode`s are managed by their owning `Context`, which has a similar enabled/disabled syntax:

    mSine->enable();
    ctx->enable();

It is important to note that enabling or disabling the Context effects the processing the entire audio graph - no audio will be processed at all if it is off and 'audio time' will not progress.  Not only can you use this to save on cpu / power when you need to, it is also a useful catch-all way to shut off the audio processing thread.

The reason why the above is true is that, although Node's are (by convention) connected source >> destination, the actual processing follows the 'pull model', i.e. destination (recursively) pulls the source.  The bulk of the work is done by ` Node::pullInputs()`, which ultimately ends up calling the virtual `Node::process()` method with the `Buffer` that should be filled with processed audio.

Other Node features include:

- can be enabled / disabled / connected / disconnected while audio is playing
- supports multiple inputs, which are implicitly summed to their specified number of channels.
- supports multiple outputs, which don't necessarily have to be connected to the Context's output( they will be added to the 'auto pull list').
- Feedback is supported by connecting Node's in a cycle, although for this to make sense there must be a Node that overrides `supportsCycles()` and returns true.  The build in `Delay` is the primary user of this feature.
- If possible (ex. one input, same # channels), a Node will process audio in-place
- Node::ChannelMode allows the channels to be decided based on either a Node's input, it's output, or specified by user.

See Also:

- [NodeBasic](../../samples/_audio/NodeBasic/src/NodeBasicApp.cpp)
- [NodeAdvanced](../../samples/_audio/NodeAdvanced/src/NodeAdvancedApp.cpp)

\subsection device_output OutputDeviceNode

The endpoint in an audio graph is currently always `OutputDeviceNode`, the Node that represents and sends audio to the hardware device on your computer. The default audio Context initializes this to a sane default when it is first accessed, with stereo channels (or mono if that is all that is available).

In order to support non device Context's in the future, `Context::getOutput()` returns an `OutputNode`, the parent class of `OutputDeviceNode`. However, you can safely typecast it, which will allow you to get at the `audio::DeviceRef` and more information related to your hardware device:

    #include "cinder/audio/OutputNode.h"

    auto ctx = ci::audio::master();
    audio::DeviceRef dev = dynamic_pointer_cast<audio::OutputDeviceNode>( ctx->getOutput() )->getDevice();
    
    ci::app::console() << "device name: " << dev->getName() << endl;
    ci::app::console() << "num output channels: " << dev->getNumOutputChannels() << endl;

There is also an easier way to get the default device, with the static `audio::Device::getDefaultOutput()` method.

### Specifying a non-default Device

If you need the Context to address a device other than the system default, you must create a LineOut with the appropriate `ci::audio::DeviceRef`, using the platform-specific virtual `Context::createOutputDeviceNode()` method. You then assign that as the master `Context`'s `OutputNode`:

    auto ctx = ci::audio::master();
    auto device = ci::audio::Device::findDeviceByName( "Device Name" );
    ci::audio::OutputDeviceNodeRef output = ctx->createOutputDeviceNode( device );
    ctx->setOutput( output );

The device name can be found in your system settings or by iterating the `DeviceRef`'s returned by `Device::getDevices()` and looking at its `getName()` property.  As an alternative to specifying the device by name, you can use `findDeviceByKey()`, which is a platform-agnostic, unique identifier that is internally generated.

### Specifying a Channel Count Other than Stereo (the default)

If you intend to handle a channel count other than the default stereo pair, you need to create a LineOut and pass in the desired channel count in its optional `Node::Format` argument. 

    auto format = ci::audio::Node::Format().channels( 10 );
    ci::audio::OutputDeviceNodeRef output = ctx->createOutputDeviceNode( device, format );
    ctx->setOutput( output );

__note__: Replacing the master `Context`'s output will cause a context-wide `Node::uninitialize()` and `Node::initialize()`.  This is because the Context controls variables that the Node's rely on, such as samplerate and frames-per-block.  While in some cases it may be unnoticeable, it's usually a good idea to call `Context::disable()` (or do a more robust halt of your graph) beforehand to prevent unexpected audio clicks.

\subsection device_input InputDeviceNode

`InputDeviceNode` is used to get microphone or other incoming hardware audio into the audio graph.  Its interface very similar to `OutputDeviceNode`, most importantly being the `getDevice()` method, which returns the owned `audio::DeviceRef`.  As with `OutputDeviceNode`, you must create the InputDeviceNode using the platform-specific virtual method `Context::createInputDeviceNode()`:

    ci::audio::InputDeviceNodeRef input = ctx->createInputDeviceNode();

    input >> ctx->getOutput();
    input->enable();

The above creates an `InputDeviceNode` with the default `Device` and default `audio::Node::Format`, giving you either stereo channel input or mono if that isn't available, and then connects it directly to the Context's output. As is the case for all `InputNode`s (`InputDeviceNode`'s parent class'), you must then call its `enable()` method before it will process audio.

If you want to force it to be mono, you can specify that with the optional format argument:

    auto format = ci::audio::Node::Format().channels( 1 );
    ci::audio::InputDeviceNodeRef input = ctx->createInputDeviceNode( Device::getDefaultInput(), format );

Of course, you can also use a non-default `Device`, as explained in the section on OutputDeviceNode.

While the above connection simply routes your microphone input the output speakers, you are able to connect any combination of effects inbetween. You can also connect the input to a `MonitorNode`, which allows you to get the audio samples back on the main thread for visualization and also doesn't require connection to the speaker output.  This is explained in more detail below.

\subsection monitor MonitorNode and MonitorSpectralNode

TODO NEXT

\subsection read_audio SourceFile and SamplePlayerNode

This section explains how to read audio files and play them back within an audio graph, either as a buffer in memory or directly from the file.

Audio files are represented by the `audio::SourceFile` class, which encapsulates a handle to the file and its properties, such as the native

TODO:  quickly document SourceFile properties and samplerate conversion

. Here is how to load a SourceFile from your assets directory:

    audio::SourceFileRef sourceFile = audio::load( loadAsset( "audiofile.wav" ) );


 The main interface for audio file playback is SamplePlayer, which is abstract and comes in two concrete flavors: `BufferPlayerNode`'s hold their audio data in-memory, and `FilePlayerNode`'s stream the audio data from file.

To create and load a `BufferPlayerNode`:

    auto ctx = audio::Context::master();
    mBufferPlayer = ctx->makeNode( new audio::BufferPlayerNode() );
    mBufferPlayer->loadBuffer( sourceFile );

Here's how to create a `FilePlayerNode`:

    mFilePlayer = ctx->makeNode( new audio::FilePlayerNOde( sourceFile ) );

Which one you use depends on your application. Usually you want to keep small audio source's, such as sound effects, in memory so that they can quickly be accessed without file i/o. The latency is also much less when playing back directly from memory.  Longer audio files, such as a soundtrack, are good candidates for reading from disk at playback time, where the latency doesn't matter as much.   

Both support reading of file types async; `BufferPlayer::loadBuffer` can be done on a background thread, and FilePlayer can be instructed to read from a background thread with an optional boolean argument. 

Supported File types:

- For mac, see file types [listed here][coreaudio-file-types].
- For windows, see file types [listed here][mediafoundation-file-types]. 
- supported ogg vorbis on all platforms.

See:

\subsection write_audio SampleRecorderNode

TODO

\subsection ChannelRouterNode ChannelRouterNode

ChannelRouterNode is used to map the channels between two connected Nodes. This can be useful in multichannel situations, or to split a stereo Node into two mono Nodes. See above for information on how to setup a multichannel OutputDeviceNode.

The following routes a `SamplePlayer` to channel 5 of the Context's output (it has already been configured to as a multi-channel output):

    auto format = ci::audio::Node::Format().channels( 10 );
    auto channelRouter = ctx->makeNode( new audio::ChannelRouterNode( format ) );
    mSamplePlayer >> mChannelRouter->route( 0, 5 );

The first argument to `ChannelRouterNode::route()` is the input channel index, the second is the output channel index.

If `mSamplePlayer` happens to be stereo, both channels will be mapped, provided that there are enough channels (starting at the ChannelRouterNode's channel index 5 ) to accomodate.  If instead you need to specifically only route a single channel, the route() method can take a third argument to specify the channel count:

    mSamplePlayer >> mChannelRouter->route( 0, 5, 1 );

See Also:

- [MultichannelOutput](../../samples/_audio/MultichannelOutput/src/MultichannelOutputApp.cpp)

See Also:

- [NodeAdvanced](../../samples/_audio/NodeAdvanced/src/NodeAdvancedApp.cpp)
- [InputAnalyzer](../../samples/_audio/InputAnalyzer/src/InputAnalyzerApp.cpp)

\subsection other_nodes Other Node's - GenNode's and Effects

TODO: document some of the nodes that ship with cinder. link to Node svg

\section param Param - sample accurate audio parameters

TODO

\section platform_specific Platform Specific Notes

_iOS Simulator:_ The iOS simulator has many problems related to hardware, limiting its usefulness for testing projects with audio.  Instead, build for mac desktop when testing.

\section other Other

\subsection Tests

While the samples demonstrate many of the techniques and tools available in a straightfoward manner, there are more exhaustive test applications for each of the various components. They are currently organized into platform-specifc workspaces:

- mac: test/_audio/Audio2Test.xcworkspace
- windows: test/_audio/Audio2Test.msw/Audio2Test.sln

These are meant to be more for feature and regression testing than anything else, but they may also be a useful way to see the entire breadth of the available functionality.

\subsection third_party Included Third Party Code

There are a few libraries written by third parties, all redistributed in source form and liberally licensed:

- [ooura] general purpose FFT algorithms.
- [r8brain] sample rate converter library, designed by Aleksey Vaneev of Voxengo.
- [oggvorbis] audio decoder / encoder for the ogg file format.

To all of the people responsible for making these available and of such high quality, thank you!


[cinder]: https://github.com/cinder/cinder
[tinderbox]: http://libcinder.org/docs/welcome/TinderBox.html
[dev-forum]: https://forum.libcinder.org/#Forum/developing-cinder
[webaudio]: https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html
[puredata]: http://puredata.info/
[coreaudio-file-types]: https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/SupportedAudioFormatsMacOSX/SupportedAudioFormatsMacOSX.html
[mediafoundation-file-types]: http://msdn.microsoft.com/en-us/library/windows/desktop/dd757927(v=vs.85).aspx
[ooura]: http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html
[r8brain]: https://code.google.com/p/r8brain-free-src/
[oggvorbis]: http://xiph.org/vorbis/
[web_audio_spec]: http://webaudio.github.io/web-audio-api/
[web_audio_modular_routing]: http://webaudio.github.io/web-audio-api/#modular-routing

*/
