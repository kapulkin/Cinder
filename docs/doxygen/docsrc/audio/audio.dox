/**
\page Audio

\tableofcontents

# ci::audio


This document provides an overview of the audio capabilities in cinder. You can use this, along with the samples in the *samples/_audio* folder, as a jumping off point into the `ci::audio` namespace.

\section design Design

The `ci::audio` namespace consists of a number of different components and layers of abstraction:

- a modular, Node-based set of real-time audio tools. 
- Voice - a higher level 'simple api' for when all you want is to play back a sound file or get a processing function.
- audio file reading and writing tools, following cinder's Source / Target pattern. 
- A lower level set of DSP tools, like FFT transform, Biquad filtering and vector based math operations.

The core of the design draws from concepts found in other popular modular audio API's, namely [Web Audio][webaudio] and [Pure Data][puredata], while combining many of cinder's existing design patterns. We also take full advantage of C++11 features such as smart pointers, `std::atomic`'s, and `std::mutex`'s.  

A modular api is advantagous because it is proven to be very flexible and allows for reusability without a significant loss in performance.  Still, higher level constructs exist and more will be added as time permits.  The cinder philosophy remains, "easy things easy and hard things possible." 

\section voice_api Voice API

For those who need only to play back a sound file or want a simple processing function, you may not need to look further than what is provided in the Voice API.  Voice's are high level objects that sit above the modular system, managing a small chain of audio Node's that perform the necessary processing.  They are meant to require little startup and management. 

The following is an example of how to play a sound file with an `audio::Voice`:

    SourceFileRef sourceFile = audio::load( loadResource( "soundfile.wav" ) );
    mVoice = audio::Voice::create( sourceFile );
    mVoice->start();

The `Voice::create` method returns a shared_ptr to a Voice sub-class (ex, in the case of playing an audio file, returns `VoiceSamplePlayerRef`, but the user will generally only need to maintain this by storing it in a VoiceRef instance variable. It is necessary to store the returned VoiceRef because once it goes out of scope, it will be disconnected from the audio graph and destroyed.  This is fairly cheap, however (much cheaper than creating the `SourceFile` via `audio::load()`, for example), so if you need to later play a different file, you can safely call `Voice::create()` again and assign it to your `mVoice` instance variable.

Common tasks like `start()`, `stop()`, and `pause()` are supported. Each Voice has controls for volume and 2d panning.  Here is an example of how you'd control these with the mouse:

    void MyApp::mouseDown( app::MouseEvent event )
    {
    	mVoice->setVolume( 1.0f - (float)event.getPos().y / (float)getWindowHeight() );
    	mVoice->setPan( (float)event.getPos().x / (float)getWindowWidth() );
    }

\subsection voice_nodes How a Voice manages its Nodes

_note: It may be beneficial to skip to the Modular API documentation before continuing on in this section._

The Voice API sits above and ties into the modular API, which is explained below. Each Voice has a virtual `Voice::getInputNode()` member function that returns the `Node` object that is responsible for generating samples. This gives you access to more advanced functionality, like setting extended properties.

Because the Voice internally manages a chain of Node's, the actual `Node` that is connected to the master output can be retrieved with the virtual `Voice::getOutputNode()` member function.  This is currently always the `Pan2d`, however a generic `NodeRef` is returned because the actual type should be opaque to the user.

\subsection voice_direct Manually directing the Voice output

By default, a `Voice` is automatically connected up to `Context::master()`, which represents the active output hardware device (ex. speakers). If you plan to direct the `Voice`'s output to something else, you need to specify this at creation time by passing `false` to `Voice::Options::connectToMaster()`. For example, the following manually connects the `Voice` up to a `Scope` (which allows one to visualize the audio samples, see docs later on), and then to `Context::master()`:

    auto options = audio::Voice::Options().connectToMaster( false );
    mVoice = audio::Voice::create( sourceFile, options );
    mScope = audio::master()->makeNode( new audio::Scope );
  
    mVoice->getOutputNode() >> mScope >> audio::master()->getOutput();

See Also:

- [VoiceBasic](../../samples/_audio/VoiceBasic/src/VoiceBasicApp.cpp)
- [VoiceBasicProcessing](../../samples/_audio/VoiceBasicProcessing/src/VoiceBasicProcessingApp.cpp)

\section modular Modular API

TODO: write intro to modular section, which explains what the hell that is.  Probably need a graph here, unfortunately.

TODO: document Buffer class upfront - it is sort of needed for everything else.

\subsection context Context

The Context class manages platform specific audio processing and thread synchronization between the 'audio' (real-time) and 'user' (typically UI/main, but not limited to) threads. There is one 'master', which is the only hardware-facing Context. All Node's are created using the Context, which is necessary for thread synchronization.  A new Node is created like the following: 

    auto ctx = audio::Context::master();
    mNode = ctx->makeNode( new NodeType );


TODO: finish documenting where samplerate and framesPerBlock comes from.

There are a couple important context-wide parameters that Node's use to configure their internal buffer layout, so the value is the same for all Node's within that Context's audio graph. The values are:

_samplerate:_ the number of processing samples per second, typically 44,100 or 48,000.
_frames per block:_: processing is sectioned up into blocks (an array of numbers, usually a power of two) to facilitate real-time operations. The default value is 512, though some systems can go much lower (like mac).

These parameters are ultimately governed by the Context's `OutputNode` (accessible with `Context::getOutput()`), which is currently always of type `OutputNodeDevice`.  This means that the samplerate and frames-per-block settings are governed by your system's hardware settings.

It is important to note that these values can change at runtime either by the user o system, which will cause the contents (Node's) of the Context to need to be reconfigured.  This should in general 'just work', though authors of Node's should be aware of this possibility when deciding how to manage their Buffers.

\subsection node Node

A Node is the fundamental building block for audio processing graphs. They allow for flexible combinations of synthesis, analysis, effects, file reading/writing, etc., and are meant to be easily subclassed. There are a two important Node types also worth mentioning upfront:

- **OutputNode**: an endpoint at the end of an audio graph. Has no outputs.
- **InputNode**: an endpoint at the beginning of an audio graph. Has no inputs.

Node's are connected together to from an audio graph. For audio to reach the speakers, the last Node in the graph is connected to the Context's OutputNode:

    auto ctx = audio::Context::master();
    mSine = ctx->makeNode( new audio::GenSineNode );
    mGain = ctx->makeNode( new audio::GainNode );
    
    mSine->connect( mGain );
    mGain->connect( ctx->getOutput() );

Node's are connected from source to destination. A convenient shorthand syntax that is meant to represent this is as follows:

    mSine >> mGain >> ctx->getOutput();

To process audio, each Node subclass implements a virtual method `process( Buffer *buffer )`. Processing can be enabled or disabled on a per-Node basis. While `NodeEffect`s are enabled by default, `NodeInput`s must be turned on before they produce any audio. `OutputNode`s are managed by their owning `Context`, which has a similar enabled/disabled syntax:

    mSine->enable();
    ctx->enable();

It is important to note that enabling or disabling the Context effects the processing the entire audio graph - no audio will be processed at all if it is off and 'audio time' will not progress.  Not only can you use this to save on cpu / power when you need to, it is also a useful catch-all way to shut off the audio processing thread.

The reason why the above is true is that, although Node's are (by convention) connected source >> destination, the actual processing follows the 'pull model', i.e. destination (recursively) pulls the source.  The bulk of the work is done by ` Node::pullInputs()`, which ultimately ends up calling the virtual `Node::process()` method with the `Buffer` that should be filled with processed audio.

Other Node features include:

* can be enabled / disabled / connected / disconnected while audio is playing
* supports multiple inputs, which are implicitly summed to their specified number of channels.
* supports multiple outputs, which don't necessarily have to be connected to the Context's output( they will be added to the 'auto pull list').
* Feedback is supported by connecting Node's in a cycle, although for this to make sense there must be a Node that overrides `supportsCycles()` and returns true.  The build in `Delay` is the primary user of this feature.
* If possible (ex. one input, same # channels), a Node will process audio in-place
* Node::ChannelMode allows the channels to be decided based on either a Node's input, it's output, or specified by user.

See Also:

- [NodeBasic](../../samples/_audio/NodeBasic/src/NodeBasicApp.cpp)
- [NodeAdvanced](../../samples/_audio/NodeAdvanced/src/NodeAdvancedApp.cpp)

\subsection device_output Device Output

The default audio Context is the one that sends audio to the default audio hardware device, and by default the number of channels set is two (or one if that is all that is available). This is most commonly accessed with `ci::audio::master()`., which returns the system's default `OutputDeviceNode`. 

###### Specifying a non-default Device

If you need the Context to address a device other than the system default, you must create a LineOut with the appropriate `ci::audio::DeviceRef`, then assign that as the master `Context`'s `OutputNode`:

    ci::audio::DeviceRef device = ci::audio::Device::findDeviceByName( "Device Name" );
    ci::audio::LineOutRef lineOut = ctx->createLineOut( device );
    ctx->setOutput( lineOut );

The device name can be found in your system settings or by iterating the `DeviceRef`'s returned by `Device::getDevices()` and looking at its `getName()` property.  As an alternative to specifying the device by name, you can use `findDeviceByKey()`, which is a platform-agnostic, unique identifier that is internally generated.

###### Specifying a Channel Count Other than Stereo (the default)

If you intend to handle a channel count other than the default stereo pair, you need to create a LineOut and pass in the desired channel count in its optional `Node::Format` argument. 

    auto format = ci::audio::Node::Format().channels( 10 );
    ci::audio::LineOutRef lineOut = ctx->createLineOut( device, format );
    ctx->setOutput( lineOut );

__note__: Replacing the master `Context`'s output will cause a context-wide `Node::uninitialize()` and `Node::initialize()`.  This is because the Context controls variables that the Node's rely on, such as samplerate and frames-per-block.  While in some cases it may be unnoticeable, it's usually a good idea to call `Context::disable()` (or do a more robust halt of your graph) beforehand to prevent unexpected audio clicks.

###### Using ChannelRouterNode to map Node's to specific output channels

While the above explains how to enable output channels greater than the standard stereo pair, addressing individual channels requires a specific Node's for this task, the `ChannelRouterNode`. This Node is used whenever you need to re-map the channels of one Node to another. For example, the following routes a `SamplePlayer` to channel 5 of the Context's output (it has been configured to as a multi-channel output like above):

    auto format = ci::audio::Node::Format().channels( 10 );
    auto channelRouter = ctx->makeNode( new audio::ChannelRouterNode( format ) );
    mSamplePlayer >> mChannelRouter->route( 0, 5 );

The first argument to `ChannelRouterNode::route()` is the input channel index, the second is the output channel index.

If `mSamplePlayer` happens to be stereo, both channels will be mapped, provided that there are enough channels (starting at the ChannelRouterNode's channel index 5 ) to accomodate.  If instead you need to specifically only route a single channel, the route() method can take a third argument to specify the channel count:

    mSamplePlayer >> mChannelRouter->route( 0, 5, 1 );

See Also:

- [MultichannelOutput](../../samples/_audio/MultichannelOutput/src/MultichannelOutputApp.cpp)

\subsection device_input Device Input

FINISH ME

\subsection read_audio Reading and Playing Audio Files

Audio files are represented by the `audio::SourceFile` class, which represents a handle to the file along with all of its properties. Here is how to load a SourceFile from your assets directory:

    mSourceFile = audio::load( loadAsset( "audiofile.wav" ) );


 The main interface for audio file playback is SamplePlayer, which is abstract and comes in two concrete flavors: `BufferPlayerNode`'s hold their audio data in-memory, and `FilePlayerNode`'s stream the audio data from file.

To create and load a `BufferPlayerNode`:

    auto ctx = audio::Context::master();
    mBufferPlayer = ctx->makeNode( new audio::BufferPlayerNode() );
    mBufferPlayer->loadBuffer( mSourceFile );

And to create a `FilePlayerNode`:

    mFilePlayer = ctx->makeNode( new audio::FilePlayerNOde( mSourceFile ) );

Both support reading of file types async; `BufferPlayer::loadBuffer` can be done on a background thread, and FilePlayer can be specified as reading from a background thread during construction. 

Supported File types:

- For mac, see file types [listed here][coreaudio-file-types].
- For windows, see file types [listed here][mediafoundation-file-types]. 
- supported ogg vorbis on all platforms.

See:

\subsection write_audio Writing Audio Files

TODO

\subsection monitor Viewing audio using MonitorNode and MonitorSpectralNode

TODO

See Also:

- [NodeAdvanced](../../samples/_audio/NodeAdvanced/src/NodeAdvancedApp.cpp)
- [InputAnalyzer](../../samples/_audio/InputAnalyzer/src/InputAnalyzerApp.cpp)

\section platform_specific Platform Specific Notes

_iOS Simulator:_ The iOS simulator has many problems related to hardware, limiting its usefulness for testing projects with audio.  Instead, build for mac desktop when testing.

\section other Other

\subsection Tests

While the samples demonstrate many of the techniques and tools available in a straightfoward manner, there are more exhaustive test applications for each of the various components. They are currently organized into platform-specifc workspaces:

- mac: test/_audio/Audio2Test.xcworkspace
- windows: test/_audio/Audio2Test.msw/Audio2Test.sln

These are meant to be more for feature and regression testing than anything else, but they may also be a useful way to see the entire breadth of the available functionality.

\subsection third_party Included Third Party Code

There are a few libraries written by third parties, all redistributed in source form and liberally licensed:

- [ooura] general purpose FFT algorithms.
- [r8brain] sample rate converter library, designed by Aleksey Vaneev of Voxengo.
- [oggvorbis] audio decoder / encoder for the ogg file format.

To all of the people responsible for making these available and of such high quality, thank you!


[cinder]: https://github.com/cinder/cinder
[tinderbox]: http://libcinder.org/docs/welcome/TinderBox.html
[dev-forum]: https://forum.libcinder.org/#Forum/developing-cinder
[webaudio]: https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html
[puredata]: http://puredata.info/
[coreaudio-file-types]: https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/SupportedAudioFormatsMacOSX/SupportedAudioFormatsMacOSX.html
[mediafoundation-file-types]: http://msdn.microsoft.com/en-us/library/windows/desktop/dd757927(v=vs.85).aspx
[ooura]: http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html
[r8brain]: https://code.google.com/p/r8brain-free-src/
[oggvorbis]: http://xiph.org/vorbis/

*/
